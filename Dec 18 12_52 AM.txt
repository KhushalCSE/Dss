prac 8
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report

# Load a sample dataset (Iris dataset)
iris = datasets.load_iris()
X = iris.data[:, :2]  # Use only the first two features for visualization
Y = iris.target

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create an SVM Classifier model
model = SVC(kernel='linear', random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions
Y_pred = model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(Y_test, Y_pred))
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred))

# Visualize the decision boundary
plt.figure(figsize=(10, 6))
h = .02  # Step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Predict on the mesh grid
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary and the training points
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', marker='o', label='Train')
plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='k', marker='s', label='Test')
plt.title('SVM Classifier Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')


prac 7
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, classification_report

# Generate sample data
np.random.seed(0)
num_samples = 100
X = np.random.rand(num_samples, 2)  # 100 samples, 2 features
# Create a binary target variable based on a simple condition
Y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary target variable

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a Decision Tree Classifier model
model = DecisionTreeClassifier(random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Make predictions
Y_pred = model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(Y_test, Y_pred))
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred))

# Visualize the decision tree
plt.figure(figsize=(12, 8))
plot_tree(model, filled=True, feature_names=['Feature 1', 'Feature 2'], class_names=['Class 0', 'Class 1'])
plt.title('Decision Tree Visualization')
plt.show()

# Visualize the results
plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_pred, cmap='coolwarm', marker='o', edgecolors='k')
plt.title('Decision Tree Predictions')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()


prac 6
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score

# Generate sample data
np.random.seed(0)
num_samples = 100
X = np.random.rand(num_samples, 3)  # 100 samples, 3 features
# Create a target variable with a linear relationship plus some noise
Y = 3 + 2 * X[:, 0] + 5 * X[:, 1] + 1 * X[:, 2] + np.random.randn(num_samples) * 0.5

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a Lasso Regression model
alpha = 0.1  # Regularization strength
model = Lasso(alpha=alpha)

# Train the model
model.fit(X_train, Y_train)

# Make predictions
Y_pred = model.predict(X_test)

# Evaluate the model
print("Mean Squared Error:", mean_squared_error(Y_test, Y_pred))
print("R^2 Score:", r2_score(Y_test, Y_pred))

# Visualize the results (only for the first two features for 2D visualization)
plt.scatter(X_test[:, 0], Y_test, color='blue', label='Actual')
plt.scatter(X_test[:, 0], Y_pred, color='red', label='Predicted')
plt.title('Lasso Regression Predictions')
plt.xlabel('Feature 1')
plt.ylabel('Target Variable')
plt.legend()
plt.show()


prac 5
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

# Generate sample data
np.random.seed(0)
num_samples = 100
X = np.random.rand(num_samples, 3)  # 100 samples, 3 features
# Create a target variable with a linear relationship plus some noise
Y = 3 + 2 * X[:, 0] + 5 * X[:, 1] + 1 * X[:, 2] + np.random.randn(num_samples) * 0.5

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a Ridge Regression model
alpha = 1.0  # Regularization strength
model = Ridge(alpha=alpha)

# Train the model
model.fit(X_train, Y_train)

# Make predictions
Y_pred = model.predict(X_test)

# Evaluate the model
print("Mean Squared Error:", mean_squared_error(Y_test, Y_pred))
print("R^2 Score:", r2_score(Y_test, Y_pred))

# Visualize the results (only for the first two features for 2D visualization)
plt.scatter(X_test[:, 0], Y_test, color='blue', label='Actual')
plt.scatter(X_test[:, 0], Y_pred, color='red', label='Predicted')
plt.title('Ridge Regression Predictions')
plt.xlabel('Feature 1')
plt.ylabel('Target Variable')
plt.legend()
plt.show()



prac 4
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report

# Generate sample data
np.random.seed(0)
num_samples = 100
X = np.random.rand(num_samples, 2)  # 100 samples, 2 features
# Create a binary target variable based on a linear combination of features
Y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary target variable

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a Logistic Regression model
model = LogisticRegression()

# Train the model
model.fit(X_train, Y_train)

# Make predictions
Y_pred = model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(Y_test, Y_pred))
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred))

# Visualize the results
plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_pred, cmap='coolwarm', marker='o', edgecolors='k')
plt.title('Logistic Regression Predictions')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()


prac 3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Generate sample data
np.random.seed(0)
num_samples = 100
X = np.random.rand(num_samples, 3)  # 100 samples, 3 features
# Create a target variable with a linear relationship
Y = 3 + 2 * X[:, 0] + 5 * X[:, 1] + 1 * X[:, 2] + np.random.randn(num_samples) * 0.5  # Adding some noise

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a Multiple Linear Regression model
model = LinearRegression()

# Train the model
model.fit(X_train, Y_train)

# Make predictions
Y_pred = model.predict(X_test)

# Evaluate the model
print("Mean Squared Error:", mean_squared_error(Y_test, Y_pred))
print("R^2 Score:", r2_score(Y_test, Y_pred))

# Visualize the results (only for the first two features for 2D visualization)
plt.scatter(X_test[:, 0], Y_test, color='blue', label='Actual')
plt.scatter(X_test[:, 0], Y_pred, color='red', label='Predicted')
plt.title('Multiple Linear Regression Predictions')
plt.xlabel('Feature 1')
plt.ylabel('Target Variable')
plt.legend()
plt.show()


prac 2
import numpy as np
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(0)
x = np.random.rand(100)
y = 2 + 3 * x + np.random.randn(100) / 1.5

# Define the simple linear regression model
class SimpleLinearRegression:
    def __init__(self):
        self.beta0 = None
        self.beta1 = None

    def fit(self, x, y):
        # Calculate the mean of x and y
        x_mean = np.mean(x)
        y_mean = np.mean(y)

        # Calculate the slope (β1) and intercept (β0)
        numerator = np.sum((x - x_mean) * (y - y_mean))
        denominator = np.sum((x - x_mean) ** 2)
        self.beta1 = numerator / denominator
        self.beta0 = y_mean - self.beta1 * x_mean

    def predict(self, x):
        return self.beta0 + self.beta1 * x

# Create an instance of the SimpleLinearRegression model
model = SimpleLinearRegression()

# Fit the model to the data
model.fit(x, y)

# Make predictions
y_pred = model.predict(x)

# Visualize the data and the regression line
plt.scatter(x, y, label='Data')
plt.plot(x, y_pred, label='Regression Line', color='red')
plt.legend()
plt.show()
plt.legend()
plt.show()